{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OlivierSERRERIperso/Lepont_exercice_fork/blob/main/Copie_de_Pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8P1cTey9mfX"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLFCpCgYT5OT",
        "outputId": "e30a118e-8255-4fba-a5b3-0a4584548d8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/316.9 MB\u001b[0m \u001b[31m916.9 kB/s\u001b[0m eta \u001b[36m0:02:32\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Wordcount avec Pyspark**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UjINajjySnIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir textes"
      ],
      "metadata": {
        "id": "mERp48wwZozu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "RyhvfQiXZyj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "# créez une session Spark avant de commencer\n",
        "spark = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
        "\n",
        "# lisez tous les fichiers texte dans un répertoire ici appelé textes\n",
        "text_files = spark.read.text(\"/content/textes/*\")\n",
        "\n",
        "# divisez les lignes en mots et effectuez le comptage des mots\n",
        "word_count = text_files.rdd.flatMap(lambda line: line[0].split(\" \")).map(lambda word: (word, 1)).reduceByKey(add) #reduce ++\n",
        "\n",
        "# affichez les résultats du comptage des mots\n",
        "result = word_count.collect()\n",
        "for (word, count) in result:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# arrêtez la session Spark avec .stop\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "kdlcRyXU-Y59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35585ab7-5194-4c77-9ec5-c85ee5c351b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tri: 5\n",
            "à: 2\n",
            "Bulles: 1\n",
            ": 2\n",
            "Compare: 1\n",
            "les: 2\n",
            "adjacents: 1\n",
            "jusqu'à: 1\n",
            "ce: 2\n",
            "que: 1\n",
            "triée.: 1\n",
            "le: 3\n",
            "petit: 1\n",
            "au: 1\n",
            "début,: 1\n",
            "répétant: 1\n",
            "processus: 1\n",
            "pour: 1\n",
            "reste: 1\n",
            "Trie: 1\n",
            "comme: 1\n",
            "si: 1\n",
            "elle: 1\n",
            "était: 1\n",
            "cartes,: 1\n",
            "insérant: 1\n",
            "appropriée.: 1\n",
            "en: 3\n",
            "deux: 3\n",
            "moitié,: 1\n",
            "Choisi: 1\n",
            "partitionne: 1\n",
            "sous-listes,: 1\n",
            "Les: 1\n",
            "lots: 1\n",
            "grands: 1\n",
            "ou: 1\n",
            ":: 5\n",
            "et: 2\n",
            "échange: 1\n",
            "éléments: 1\n",
            "la: 5\n",
            "liste: 4\n",
            "soit: 1\n",
            "par: 2\n",
            "Sélection: 1\n",
            "Sélectionne: 1\n",
            "plus: 1\n",
            "élément: 3\n",
            "place: 2\n",
            "de: 5\n",
            "liste.: 1\n",
            "Insertion: 1\n",
            "une: 1\n",
            "main: 1\n",
            "chaque: 3\n",
            "sa: 1\n",
            "Fusion: 1\n",
            "Divise: 1\n",
            "moitiés,: 1\n",
            "trie: 2\n",
            "puis: 2\n",
            "fusionne: 1\n",
            "moitiés: 1\n",
            "triées.: 1\n",
            "Rapide: 1\n",
            "un: 1\n",
            "pivot,: 1\n",
            "récursivement: 1\n",
            "sous-liste.: 1\n",
            "Big: 1\n",
            "Data: 1\n",
            "concernent: 1\n",
            "l'analyse: 1\n",
            "ensembles: 1\n",
            "données: 1\n",
            "manière: 1\n",
            "périodique: 1\n",
            "planifiée: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Je veux supprimer les StopWords:"
      ],
      "metadata": {
        "id": "skesrPHraoMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "# créez une session Spark\n",
        "spark = SparkSession.builder.appName(\"WordCountSansStopWords\").getOrCreate()\n",
        "\n",
        "# lisez tous les fichiers texte dans un répertoire\n",
        "text_files = spark.read.text(\"/content/textes/*\")\n",
        "\n",
        "# liste des mots vides en français si le texte est en francais bien sur\n",
        "stop_words = [\"le\", \"la\", \"les\", \"de\", \"du\", \"des\", \"et\", \"à\", \"un\", \"une\", \"ce\", \"cette\"]\n",
        "\n",
        "# divisez les lignes en mots\n",
        "#filtrez les mots vides (on en a pas besoin) et effectuez le comptage des mots\n",
        "word_count = (\n",
        "    text_files.rdd\n",
        "    .flatMap(lambda line: line[0].split(\" \"))\n",
        "    .filter(lambda word: word.lower() not in stop_words)\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(add)\n",
        ")\n",
        "\n",
        "# affichez ici les résultats du comptage des mots sans les mots vides\n",
        "result = word_count.collect()\n",
        "for (word, count) in result:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# arrêtez la session Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxlWs4NYa_tY",
        "outputId": "2de4868b-b632-4366-b01f-a4c297f87bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tri: 5\n",
            "Bulles: 1\n",
            ": 2\n",
            "Compare: 1\n",
            "adjacents: 1\n",
            "jusqu'à: 1\n",
            "que: 1\n",
            "triée.: 1\n",
            "petit: 1\n",
            "au: 1\n",
            "début,: 1\n",
            "répétant: 1\n",
            "processus: 1\n",
            "pour: 1\n",
            "reste: 1\n",
            "Trie: 1\n",
            "comme: 1\n",
            "si: 1\n",
            "elle: 1\n",
            "était: 1\n",
            "cartes,: 1\n",
            "insérant: 1\n",
            "appropriée.: 1\n",
            "en: 3\n",
            "deux: 3\n",
            "moitié,: 1\n",
            "Choisi: 1\n",
            "partitionne: 1\n",
            "sous-listes,: 1\n",
            "lots: 1\n",
            "grands: 1\n",
            "ou: 1\n",
            ":: 5\n",
            "échange: 1\n",
            "éléments: 1\n",
            "liste: 4\n",
            "soit: 1\n",
            "par: 2\n",
            "Sélection: 1\n",
            "Sélectionne: 1\n",
            "plus: 1\n",
            "élément: 3\n",
            "place: 2\n",
            "liste.: 1\n",
            "Insertion: 1\n",
            "main: 1\n",
            "chaque: 3\n",
            "sa: 1\n",
            "Fusion: 1\n",
            "Divise: 1\n",
            "moitiés,: 1\n",
            "trie: 2\n",
            "puis: 2\n",
            "fusionne: 1\n",
            "moitiés: 1\n",
            "triées.: 1\n",
            "Rapide: 1\n",
            "pivot,: 1\n",
            "récursivement: 1\n",
            "sous-liste.: 1\n",
            "Big: 1\n",
            "Data: 1\n",
            "concernent: 1\n",
            "l'analyse: 1\n",
            "ensembles: 1\n",
            "données: 1\n",
            "manière: 1\n",
            "périodique: 1\n",
            "planifiée: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 10 Words"
      ],
      "metadata": {
        "id": "ihEzhbaibyyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "# créez une session Spark\n",
        "spark = SparkSession.builder.appName(\"WordCountTop5\").getOrCreate()\n",
        "\n",
        "# lisez tous les fichiers texte dans un répertoire\n",
        "text_files = spark.read.text(\"/content/textes/*\")\n",
        "\n",
        "# liste des mots vides en français\n",
        "stop_words = [\"le\", \"la\", \"les\", \"de\", \"du\", \"des\", \"et\", \"à\", \"un\", \"une\", \"ce\", \"cette\"]\n",
        "\n",
        "# divisez les lignes en mots, filtrez les mots vides et effectuez le comptage des mots\n",
        "word_count = (\n",
        "    text_files.rdd\n",
        "    .flatMap(lambda line: line[0].split(\" \"))\n",
        "    .filter(lambda word: word.lower() not in stop_words)\n",
        "    .map(lambda word: (word, 1))\n",
        "    .reduceByKey(add)\n",
        ")\n",
        "\n",
        "# triez les résultats par ordre décroissant de fréquence\n",
        "sorted_word_count = word_count.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "# affichez les résultats du comptage des mots sans les mots vides\n",
        "result = sorted_word_count.collect()\n",
        "for (word, count) in result:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# affichez les cinq mots les plus fréquents ----> selon le choix\n",
        "top_5 = sorted_word_count.take(5)\n",
        "print(\"\\nTop 5 mots les plus fréquents :\")\n",
        "for (word, count) in top_5:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "# arrêtez la session Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to6xXoNMbyL6",
        "outputId": "bf9a1cc2-d712-4ed7-99a7-da4807d6919d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tri: 5\n",
            ":: 5\n",
            "liste: 4\n",
            "en: 3\n",
            "deux: 3\n",
            "élément: 3\n",
            "chaque: 3\n",
            ": 2\n",
            "par: 2\n",
            "place: 2\n",
            "trie: 2\n",
            "puis: 2\n",
            "Bulles: 1\n",
            "Compare: 1\n",
            "adjacents: 1\n",
            "jusqu'à: 1\n",
            "que: 1\n",
            "triée.: 1\n",
            "petit: 1\n",
            "au: 1\n",
            "début,: 1\n",
            "répétant: 1\n",
            "processus: 1\n",
            "pour: 1\n",
            "reste: 1\n",
            "Trie: 1\n",
            "comme: 1\n",
            "si: 1\n",
            "elle: 1\n",
            "était: 1\n",
            "cartes,: 1\n",
            "insérant: 1\n",
            "appropriée.: 1\n",
            "moitié,: 1\n",
            "Choisi: 1\n",
            "partitionne: 1\n",
            "sous-listes,: 1\n",
            "lots: 1\n",
            "grands: 1\n",
            "ou: 1\n",
            "échange: 1\n",
            "éléments: 1\n",
            "soit: 1\n",
            "Sélection: 1\n",
            "Sélectionne: 1\n",
            "plus: 1\n",
            "liste.: 1\n",
            "Insertion: 1\n",
            "main: 1\n",
            "sa: 1\n",
            "Fusion: 1\n",
            "Divise: 1\n",
            "moitiés,: 1\n",
            "fusionne: 1\n",
            "moitiés: 1\n",
            "triées.: 1\n",
            "Rapide: 1\n",
            "pivot,: 1\n",
            "récursivement: 1\n",
            "sous-liste.: 1\n",
            "Big: 1\n",
            "Data: 1\n",
            "concernent: 1\n",
            "l'analyse: 1\n",
            "ensembles: 1\n",
            "données: 1\n",
            "manière: 1\n",
            "périodique: 1\n",
            "planifiée: 1\n",
            "\n",
            "Top 5 mots les plus fréquents :\n",
            "Tri: 5\n",
            ":: 5\n",
            "liste: 4\n",
            "en: 3\n",
            "deux: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarité de Jaccard entre ces ensembles**:\n",
        "\n",
        "La similarité de Jaccard est une mesure statistique utilisée pour évaluer la similitude entre deux ensembles. Elle est définie comme le rapport de la taille de l'intersection de deux ensembles sur la taille de leur union. La formule mathématique de la similarité de Jaccard (J) entre deux ensembles $A$ et $B$ est la suivante :\n",
        "\n",
        " $ J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|} $"
      ],
      "metadata": {
        "id": "3mHOuL7slsDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# créez une session Spark\n",
        "spark = SparkSession.builder.appName(\"SimilariteJaccardAvecPySpark\").getOrCreate()\n",
        "\n",
        "# deux ensembles de mots\n",
        "ensemble1 = set([\"apple\", \"orange\", \"banana\", \"grape\", \"kiwi\"])\n",
        "ensemble2 = set([\"banana\", \"kiwi\", \"watermelon\", \"grape\", \"pear\"])\n",
        "\n",
        "# créez des RDD à partir des ensembles\n",
        "rdd1 = spark.sparkContext.parallelize(ensemble1)\n",
        "rdd2 = spark.sparkContext.parallelize(ensemble2)\n",
        "\n",
        "# calculez la taille des ensembles\n",
        "taille_ensemble1 = rdd1.count()\n",
        "taille_ensemble2 = rdd2.count()\n",
        "\n",
        "# calculez la taille de l'intersection des ensembles\n",
        "intersection_taille = rdd1.intersection(rdd2).count()\n",
        "\n",
        "# calculez la taille de l'union des ensembles\n",
        "union_taille = rdd1.union(rdd2).distinct().count()\n",
        "\n",
        "# calculez la similarité de Jaccard\n",
        "similarite_jaccard = intersection_taille / union_taille\n",
        "\n",
        "# affichez les résultats de la recherche\n",
        "print(f\"Ensemble 1: {ensemble1}\")\n",
        "print(f\"Ensemble 2: {ensemble2}\")\n",
        "print(f\"Taille de l'intersection: {intersection_taille}\")\n",
        "print(f\"Taille de l'union: {union_taille}\")\n",
        "print(f\"Similarité de Jaccard: {similarite_jaccard}\")\n",
        "\n",
        "# arrêtez la session Spark\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_rsPZ5nlpPN",
        "outputId": "f3236db6-cdf8-4a85-8737-4486f0bce71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble 1: {'orange', 'grape', 'kiwi', 'banana', 'apple'}\n",
            "Ensemble 2: {'grape', 'watermelon', 'pear', 'kiwi', 'banana'}\n",
            "Taille de l'intersection: 3\n",
            "Taille de l'union: 7\n",
            "Similarité de Jaccard: 0.42857142857142855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prédiction**"
      ],
      "metadata": {
        "id": "exjOt3I9obGy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LhAjEFKoaaa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}